{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaxGSEO/Cloud-page-semantic-publishing/blob/main/GSC_Analysis_%2B_Content_Decay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
    
      "metadata": {
        "id": "ZCVMI-acHzTS"
      },
      "id": "ZCVMI-acHzTS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Search Console Analysis (Use Case 2)\n"
      ],
      "metadata": {
        "id": "vxbjYDt5Ea-x"
      },
      "id": "vxbjYDt5Ea-x"
    },
    {
      "cell_type": "code",
      "source": [
        "#install this to save plotly pictures\n",
        "%%capture\n",
        "!pip install advertools plotly -U kaleido"
      ],
      "metadata": {
        "id": "CDxVjE2SbS9r"
      },
      "id": "CDxVjE2SbS9r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may need to restart your runtime after this installation.\n",
        "\n",
        "From the menu above, Runtime > Restart runtime and ignore this cell!\n",
        "\n"
      ],
      "metadata": {
        "id": "RvUANN80dZmO"
      },
      "id": "RvUANN80dZmO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13fa2ec3-c41f-4754-9c25-d229607e0b03",
      "metadata": {
        "id": "13fa2ec3-c41f-4754-9c25-d229607e0b03"
      },
      "outputs": [],
      "source": [
        "# data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# viz\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import kaleido.scopes.plotly as kaleido\n",
        "\n",
        "# utils\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please recall to change \"sample.csv\" with the name of the CSV file you have uploaded.\n",
        "\n",
        "How to upload a file? It's explained in the ebook."
      ],
      "metadata": {
        "id": "w5qHZEIfEi5z"
      },
      "id": "w5qHZEIfEi5z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a9c7261-59b9-4c75-8d83-485cd4604cb9",
      "metadata": {
        "id": "9a9c7261-59b9-4c75-8d83-485cd4604cb9"
      },
      "outputs": [],
      "source": [
        "#upload your GSC file from the API\n",
        "main = pd.read_csv(\"data_sample.csv\")\n",
        "#create copy of dataframe just in case\n",
        "df = main.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcfc4ce9-1610-4e23-baee-95907b29109f",
      "metadata": {
        "id": "bcfc4ce9-1610-4e23-baee-95907b29109f"
      },
      "outputs": [],
      "source": [
        "#number of rows of the dataframe\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af2825ff-985d-4e6a-89bc-1c40140a55ef",
      "metadata": {
        "id": "af2825ff-985d-4e6a-89bc-1c40140a55ef"
      },
      "source": [
        "# Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f7bac3-c133-458e-8f2f-b7c820d584c8",
      "metadata": {
        "id": "20f7bac3-c133-458e-8f2f-b7c820d584c8"
      },
      "outputs": [],
      "source": [
        "#remove all URLs containing the following strings. You can add/remove them based on the website you are analyzing.\n",
        "filter_page = ['category', '#', 'blog', 'tag', 'author', 'wp-content', 'upload', 'page']\n",
        "\n",
        "df = df[~df['page'].str.contains(\"|\".join(filter_page))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfd18070-21f8-486c-838a-bd18df1568d9",
      "metadata": {
        "id": "bfd18070-21f8-486c-838a-bd18df1568d9"
      },
      "outputs": [],
      "source": [
        "#removing foreign characters\n",
        "def is_ascii(s):\n",
        "    try:\n",
        "        s.encode(encoding='utf-8').decode('ascii')\n",
        "    except UnicodeDecodeError:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "#this line of code applies the function to the query column of the dataframe.\n",
        "df = df[df['query'].map(lambda x: x.isascii())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "280be059-10e7-498e-88ec-ef1953a4deb8",
      "metadata": {
        "id": "280be059-10e7-498e-88ec-ef1953a4deb8"
      },
      "outputs": [],
      "source": [
        "len(df.page.unique())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(['query', 'page', 'date'], inplace=True)"
      ],
      "metadata": {
        "id": "QuHfxcUgcuPY"
      },
      "id": "QuHfxcUgcuPY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e2ebe55e-fd96-43fb-b3ae-7c55bdbc5da8",
      "metadata": {
        "tags": [],
        "id": "e2ebe55e-fd96-43fb-b3ae-7c55bdbc5da8"
      },
      "source": [
        "# Creating new metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef049f2-d779-4165-9557-776cb53ab711",
      "metadata": {
        "id": "0ef049f2-d779-4165-9557-776cb53ab711"
      },
      "outputs": [],
      "source": [
        "#keywords a page is ranking for\n",
        "df['n_kws_page_ranks_for'] = df['page'].map(df.groupby('page')['page'].count())\n",
        "rows = len(df)\n",
        "df['percent_kws_page_ranks_for'] = (df['n_kws_page_ranks_for'] / rows * 100).round(2)\n",
        "\n",
        "#feature engineering\n",
        "df.fillna({\"impressions\": 0, \"clicks\": 0, \"position\": 0}, inplace=True)\n",
        "df = df.astype({\"impressions\": int, \"clicks\": int, \"position\": int})\n",
        "#create new colummns\n",
        "df['total_clicks_page'] = df['clicks'].groupby(df['page']).transform('sum')\n",
        "df['total_page_imps_percent'] = df['impressions'].groupby(df['page']).transform('sum')\n",
        "#percentages\n",
        "df['total_page_clicks_percent'] = (df['total_clicks_page'] / sum(df['clicks']) * 100).round(2)\n",
        "df['page_percent_imps'] = (df['total_page_imps_percent'] / sum(df['impressions']) * 100).round(2)\n",
        "df = df.round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c67f111-6bf0-4d24-be87-74443a51eb62",
      "metadata": {
        "id": "8c67f111-6bf0-4d24-be87-74443a51eb62"
      },
      "source": [
        "# Get leading queries by clicks or impressions (i.e. the top query by page)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can add even more columns to enhance our analysis. I love inserting 2 columns to find what are the best queries for every page.\n",
        "\n",
        "This allows us to understand if the page is \"matching\" its intent.\n",
        "\n",
        "E.g. An article not performing well will show queries that are barely related, in most cases."
      ],
      "metadata": {
        "id": "vG8m7NEIFDAZ"
      },
      "id": "vG8m7NEIFDAZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e485d832-77d8-492b-ac86-c63207e2135a",
      "metadata": {
        "id": "e485d832-77d8-492b-ac86-c63207e2135a"
      },
      "outputs": [],
      "source": [
        "#add leading queries\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Group the data by the URL and sort the queries by the number of clicks\n",
        "df_clicks = df.groupby('page')['query', 'clicks'].apply(lambda x: x.sort_values('clicks', ascending=False))\n",
        "\n",
        "# Select the first row of each group (the query with the highest number of clicks)\n",
        "df_clicks = df_clicks.groupby('page').first()\n",
        "\n",
        "\n",
        "# Group the data by the URL and sort the queries by the number of impressions\n",
        "df_impressions = df.groupby('page')['query', 'impressions'].apply(lambda x: x.sort_values('impressions', ascending=False))\n",
        "\n",
        "# Select the first row of each group (the query with the highest number of impressions)\n",
        "df_impressions = df_impressions.groupby('page').first()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c926b53-66bd-416b-8eba-f0c3e22163f1",
      "metadata": {
        "id": "4c926b53-66bd-416b-8eba-f0c3e22163f1"
      },
      "outputs": [],
      "source": [
        "df['leading_query_by_clicks'] = df['page'].map(df_clicks['query'])\n",
        "df['leading_query_by_impressions'] = df['page'].map(df_impressions['query'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8d11bf4-c995-4ecd-a897-3c61f007e5a6",
      "metadata": {
        "id": "e8d11bf4-c995-4ecd-a897-3c61f007e5a6"
      },
      "source": [
        "# Some DataViz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply Sturges' Rule to find the ideal number of bins for the histogram."
      ],
      "metadata": {
        "id": "fPhj4FvGFcmR"
      },
      "id": "fPhj4FvGFcmR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48141a4e-4b5a-4a65-bd4d-c228f4a175bd",
      "metadata": {
        "id": "48141a4e-4b5a-4a65-bd4d-c228f4a175bd"
      },
      "outputs": [],
      "source": [
        "# find number of bins with Sturges' Rule\n",
        "n_bins = 1 + int(np.log(len(df)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The snippet of code below is classifiyng queries according to how many clicks they get.\n",
        "\n",
        "To do so, we have to group by query first and then sum clicks."
      ],
      "metadata": {
        "id": "f_xsyTNcFmVK"
      },
      "id": "f_xsyTNcFmVK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b84ac27-ded0-46f4-821d-2ea9d8894933",
      "metadata": {
        "id": "9b84ac27-ded0-46f4-821d-2ea9d8894933"
      },
      "outputs": [],
      "source": [
        "#create bands based on clicks. Having more values in low groups means you are weaker to avg. position drops.\n",
        "def label_bands (row):\n",
        "   if row['clicks'] == 0:\n",
        "      return '0-click'\n",
        "   if row['clicks'] == 1:\n",
        "      return '1-click'\n",
        "   if 2 <= row['clicks'] <= 20:\n",
        "      return '2-to-20'\n",
        "   if 21 <= row['clicks'] <= 100:\n",
        "      return '21-100'\n",
        "   return '100+'\n",
        "\n",
        "grpd = df.groupby([\"query\"]).agg({\"clicks\": \"sum\"}).sort_values(by=\"clicks\", ascending=False)\n",
        "grpd['clicks_category'] = grpd.apply(lambda x: label_bands(x), axis=1)\n",
        "df = df.merge(right=grpd.reset_index()[['query', 'clicks_category']], on=\"query\", how=\"inner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same operation for Impressions."
      ],
      "metadata": {
        "id": "8bUX4ETfF0lJ"
      },
      "id": "8bUX4ETfF0lJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a56af6f-bb5d-43f4-8a8f-b2a493c19ad8",
      "metadata": {
        "id": "7a56af6f-bb5d-43f4-8a8f-b2a493c19ad8"
      },
      "outputs": [],
      "source": [
        "#create bands based on impressions.\n",
        "def label_bands_impressions (row):\n",
        "   if row['impressions'] == 1:\n",
        "      return '1-imp'\n",
        "   if 2 <= row['impressions'] <= 20:\n",
        "      return '2-to-20'\n",
        "   if 21 <= row['impressions'] <= 100:\n",
        "      return '21-100'\n",
        "   return '100+'\n",
        "\n",
        "grpd = df.groupby([\"query\"]).agg({\"impressions\": \"sum\"}).sort_values(by=\"impressions\", ascending=False)\n",
        "grpd['impressions_category'] = grpd.apply(lambda x: label_bands_impressions(x), axis=1)\n",
        "df = df.merge(right=grpd.reset_index()[['query', 'impressions_category']], on=\"query\", how=\"inner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are ready to plot a histogram displaying a full breakdown of the queries on a website."
      ],
      "metadata": {
        "id": "Iei3K_nBF2aF"
      },
      "id": "Iei3K_nBF2aF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a19229-b5a9-480e-a8d4-ad7b9e305b0a",
      "metadata": {
        "id": "b7a19229-b5a9-480e-a8d4-ad7b9e305b0a"
      },
      "outputs": [],
      "source": [
        "clicks_band_hist = px.histogram(\n",
        "    data_frame=df.drop_duplicates(['query', 'clicks_category', 'impressions_category']),\n",
        "    x='clicks_category',\n",
        "    y='query',\n",
        "    nbins=n_bins,\n",
        "    title=\"Query Count by Clicks - Finding opportunities + Assessing Status\",\n",
        "    labels={'clicks_category': 'Clicks Band', 'count':'Frequency'},\n",
        "    template='plotly_dark',\n",
        "    width=1024,\n",
        "    height=600, histfunc=\"count\", color=\"impressions_category\"\n",
        ")\n",
        "clicks_band_hist.update_layout( # customize font and legend orientation & position\n",
        "    font_family=\"Inter\",\n",
        "    legend=dict(\n",
        "        title=None, orientation=\"h\", y=1, yanchor=\"bottom\", x=0.5, xanchor=\"center\",\n",
        "    ),\n",
        "    xaxis={'categoryorder':'total descending'}\n",
        ")\n",
        "clicks_band_hist.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also measure query count over time with a simple line plot."
      ],
      "metadata": {
        "id": "yRNIJnkOGG5S"
      },
      "id": "yRNIJnkOGG5S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f086bf-d3db-4559-a930-a2dc2e7a53d1",
      "metadata": {
        "id": "b0f086bf-d3db-4559-a930-a2dc2e7a53d1"
      },
      "outputs": [],
      "source": [
        "df['month'] = df.date.dt.month\n",
        "\n",
        "def plot_clicks_by_month(df):\n",
        "  # create a new column with the month for each date\n",
        "\n",
        "  # group the data by query band and month, and sum the number of clicks\n",
        "    df = df.groupby(['clicks_category', 'month'])['query'].nunique().reset_index()\n",
        "\n",
        "  # use Plotly to create a line chart with the sum of clicks on the y-axis\n",
        "  # and the month on the x-axis, and a separate line for each query band\n",
        "    fig = px.line(df, x='month', y='query', color='clicks_category',\n",
        "               template='plotly_dark', width=1024, height=600, labels={'month': 'Month', 'query':'Unique query count'})\n",
        "    fig.update_xaxes(dtick=\"M1\", tickformat=\"%Y-%m\")\n",
        "    fig.update_yaxes(autorange=True)\n",
        "    fig.show()\n",
        "    fig.write_image(\"query_over_time.png\")\n",
        "\n",
        "plot_clicks_by_month(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33502e8f-ab3f-48cb-9f28-8a3c372cac18",
      "metadata": {
        "id": "33502e8f-ab3f-48cb-9f28-8a3c372cac18"
      },
      "source": [
        "# Queries with 0 clicks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8975be4b-718e-4985-8cc0-37d970bc35e6",
      "metadata": {
        "id": "8975be4b-718e-4985-8cc0-37d970bc35e6"
      },
      "outputs": [],
      "source": [
        "df_zero_query = df.loc[df['clicks_category'] == '0-click'][['query','position']]\n",
        "df_zero_query.drop_duplicates(['query'], inplace=True)\n",
        "df_zero_query.to_csv(\"zero_clicks_queries.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c073c5-9f6d-43d6-9de5-5fc31258f22b",
      "metadata": {
        "id": "94c073c5-9f6d-43d6-9de5-5fc31258f22b"
      },
      "source": [
        "# Find bi/tri-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d1a7109-8b6e-4eca-b2ee-2243d6813314",
      "metadata": {
        "id": "5d1a7109-8b6e-4eca-b2ee-2243d6813314"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "c_vec = CountVectorizer(stop_words=stoplist, ngram_range=(2,3))\n",
        "# matrix of ngrams\n",
        "ngrams = c_vec.fit_transform(df[df['position'] < 10].sort_values(\"clicks\", ascending=True)['query'].head(500))\n",
        "# count frequency of ngrams\n",
        "count_values = ngrams.toarray().sum(axis=0)\n",
        "# list of ngrams\n",
        "vocab = c_vec.vocabulary_\n",
        "df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
        "           ).rename(columns={0: 'frequency', 1:'bigram/trigram'})\n",
        "\n",
        "# Calculate cosine similarity between ngrams\n",
        "similarity_matrix = cosine_similarity(ngrams.T)\n",
        "\n",
        "# Set a similarity threshold\n",
        "similarity_threshold = 0.8\n",
        "\n",
        "# Create a list of similar ngrams to remove\n",
        "ngrams_to_remove = set()\n",
        "for i in range(len(similarity_matrix)):\n",
        "    for j in range(i+1, len(similarity_matrix)):\n",
        "        if similarity_matrix[i,j] > similarity_threshold:\n",
        "            ngrams_to_remove.add(i)\n",
        "            ngrams_to_remove.add(j)\n",
        "\n",
        "# Remove similar ngrams from the dataframe\n",
        "df_ngram_filtered = df_ngram.loc[~np.isin(range(len(df_ngram)), list(ngrams_to_remove))]\n",
        "df_ngram_filtered.head(100).style.background_gradient()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a7252d-cd6c-496b-b88c-cfae939cc5c7",
      "metadata": {
        "id": "e5a7252d-cd6c-496b-b88c-cfae939cc5c7"
      },
      "source": [
        "# Percentage of 0-clicks pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e76e0b4-c03f-4ca0-93fb-63780eb3962d",
      "metadata": {
        "id": "4e76e0b4-c03f-4ca0-93fb-63780eb3962d"
      },
      "outputs": [],
      "source": [
        "total = len(df.groupby('page').clicks.sum())\n",
        "zero_clicks = len(np.where(df.groupby('page').clicks.sum() == 0)[0])\n",
        "zero_clicks_perc = round( (zero_clicks / total) * 100, 2)\n",
        "zero_clicks_perc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bb40e3b-1883-4e56-95aa-0a6ea79d2c0e",
      "metadata": {
        "id": "7bb40e3b-1883-4e56-95aa-0a6ea79d2c0e"
      },
      "outputs": [],
      "source": [
        "#create bands based on clicks. Having more values in low groups means you are weaker to avg. position drops.\n",
        "def label_bands (row):\n",
        "   if row['clicks'] == 0:\n",
        "      return '0-click'\n",
        "   if row['clicks'] == 1:\n",
        "      return '1-click'\n",
        "   if 2 <= row['clicks'] <= 20:\n",
        "      return '2-to-20'\n",
        "   if 21 <= row['clicks'] <= 100:\n",
        "      return '21-100'\n",
        "   return '100+'\n",
        "\n",
        "grpd = df.groupby([\"page\"]).agg({\"clicks\": \"sum\"}).sort_values(by=\"clicks\", ascending=False)\n",
        "grpd['clicks_pages'] = grpd.apply(lambda x: label_bands(x), axis=1)\n",
        "df = df.merge(right=grpd.reset_index()[['page', 'clicks_pages']], on=\"page\", how=\"inner\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "330e9063-6cae-4103-9a33-113eed5edf4a",
      "metadata": {
        "id": "330e9063-6cae-4103-9a33-113eed5edf4a"
      },
      "outputs": [],
      "source": [
        "zero_pages = pd.Series(df[df['clicks_pages'] == \"0-click\"]['page'].unique())\n",
        "zero_pages.to_csv(\"zero_pages.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ab5999-8a50-45f9-beb3-22377562f081",
      "metadata": {
        "id": "77ab5999-8a50-45f9-beb3-22377562f081"
      },
      "outputs": [],
      "source": [
        "top_10_pages_clicks_sum = df.groupby('page').clicks.sum().sort_values(ascending=False).head(10).sum()\n",
        "all_clicks_sum =  df.groupby('page').clicks.sum().sum()\n",
        "top_10_pages_clicks_perc = round((top_10_pages_clicks_sum / all_clicks_sum) * 100, 2)\n",
        "top_10_pages_clicks_perc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What's the contribution of the top 10 pages (by Clicks)?"
      ],
      "metadata": {
        "id": "i6UjxdmD18l5"
      },
      "id": "i6UjxdmD18l5"
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_pages_clicks_sum = df.groupby('page').clicks.sum().sort_values(ascending=False).head(10).sum()\n",
        "all_clicks_sum =  df.groupby('page').clicks.sum().sum()\n",
        "top_10_pages_clicks_perc = round((top_10_pages_clicks_sum / all_clicks_sum) * 100, 2)\n",
        "top_10_pages_clicks_perc"
      ],
      "metadata": {
        "id": "Mq1la7j-2G_B"
      },
      "id": "Mq1la7j-2G_B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#you can also get the contribution of the top 1%. This refers to the 99th percentile!\n",
        "top_1_perc = np.percentile(df.groupby('page').clicks.sum(), 99) # Calculate the 99th percentile\n",
        "top_1_pages_clicks_sum = df.groupby('page').clicks.sum()[df.groupby('page').clicks.sum() >= top_1_perc].sum() # Calculate the clicks generated by pages in the top 1%\n",
        "all_clicks_sum =  df.groupby('page').clicks.sum().sum()\n",
        "top_1_pages_clicks_perc = round((top_1_pages_clicks_sum / all_clicks_sum) * 100, 2)\n",
        "top_1_pages_clicks_perc"
      ],
      "metadata": {
        "id": "mHJmhjkb2jOW"
      },
      "id": "mHJmhjkb2jOW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "23f0279c-b519-44e4-8b5f-cdc18beecec2",
      "metadata": {
        "id": "23f0279c-b519-44e4-8b5f-cdc18beecec2"
      },
      "source": [
        "# Heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A heatmap is just a way to visualize a pivot table.\n",
        "\n",
        "You can even skip it but I don't recommend doing it because it looks cooler."
      ],
      "metadata": {
        "id": "GJu0CmzIGPE9"
      },
      "id": "GJu0CmzIGPE9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53ab7ff5-ab30-4f04-81b7-22d35931c37f",
      "metadata": {
        "id": "53ab7ff5-ab30-4f04-81b7-22d35931c37f"
      },
      "outputs": [],
      "source": [
        "#please recall that we created a month variable from date in Use Case 2.\n",
        "df.date = pd.to_datetime(df.date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c312dc0d-b10c-4b54-898e-9727e0c337cd",
      "metadata": {
        "id": "c312dc0d-b10c-4b54-898e-9727e0c337cd"
      },
      "outputs": [],
      "source": [
        "#keeping all queries which position is less than or equal to 10. N.B. this is a quick and inaccurate way to filter.\n",
        "#hm_data_qc_pos_in_time = df.query('position <= 10').pivot_table(index='position', columns='month', values = \"query\", aggfunc='count', fill_value=0)\n",
        "#this line of code takes unique queries into account. You can use the above line if you want to consider all of them instead.\n",
        "hm_data_qc_pos_in_time = df.query('position <= 10').pivot_table(index='position', columns='month', values = \"query\", aggfunc=lambda x: len(set(x)), fill_value=0)\n",
        "#renaming numbers into month names\n",
        "hm_data_qc_pos_in_time.rename(columns={\n",
        "    1: 'January',\n",
        "    2: 'February',\n",
        "    3: 'March',\n",
        "    4: 'April',\n",
        "    5: 'May',\n",
        "    6: 'June',\n",
        "    7: 'July',\n",
        "    8: 'August',\n",
        "    9: 'September',\n",
        "    10: 'October',\n",
        "    11: 'November',\n",
        "    12: 'December'\n",
        "}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50313539-7c07-4cab-aff7-3c300689c8f4",
      "metadata": {
        "id": "50313539-7c07-4cab-aff7-3c300689c8f4"
      },
      "outputs": [],
      "source": [
        "heatmap = px.imshow(hm_data_qc_pos_in_time, text_auto=True)\n",
        "heatmap.update_layout(\n",
        "     title='Change of positions over time',\n",
        "     yaxis = dict(\n",
        "        tickmode = 'array',\n",
        "        tickvals = [i for i in np.arange(11)],\n",
        "    ),\n",
        "     template='plotly_dark',\n",
        "     xaxis_title='',\n",
        "     yaxis_title='Rankings',\n",
        "     width=1024,\n",
        "     height=600,\n",
        "     legend=dict(\n",
        "        yanchor=\"top\",\n",
        "        y=0.99,\n",
        "        xanchor=\"left\",\n",
        "        x=0.01\n",
        "        ))\n",
        "heatmap.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifying Pages Based On Performance"
      ],
      "metadata": {
        "id": "X14oOlEZ29Pr"
      },
      "id": "X14oOlEZ29Pr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideally, you shouldn't give advice based on all the pages of a website. Why?\n",
        "\n",
        "Because it's not actionable.\n",
        "\n",
        "This is why I want to show you a basic but effective way of classifying pages.\n",
        "\n",
        "P.S. The idea below was inspired by [Daniel Foley Carter](https://seo-audits.io/).\n"
      ],
      "metadata": {
        "id": "xb-7JKMw3BnD"
      },
      "id": "xb-7JKMw3BnD"
    },
    {
      "cell_type": "code",
      "source": [
        "#create bands based on clicks. Having more values in low groups means you are weaker to avg. position drops.\n",
        "def label_bands (row):\n",
        "   if row['clicks'] > 1000:\n",
        "      return 'Top'\n",
        "   elif 101 <= row['clicks'] <= 1000 :\n",
        "      return 'Good'\n",
        "   elif 21 <= row['clicks'] <= 100:\n",
        "      return 'Fair'\n",
        "   elif 1 <= row['clicks'] <= 20:\n",
        "      return 'Weak'\n",
        "   elif row ['clicks'] == 0 and row['impressions'] > 100:\n",
        "      return 'Opportunity'\n",
        "   elif row ['clicks'] == 0 and row['impressions'] <= 100:\n",
        "      return 'Dead'\n",
        "\n",
        "grpd = df.groupby([\"page\"]).agg({\"clicks\": \"sum\", \"impressions\":\"sum\"}).sort_values(by=\"clicks\", ascending=False)\n",
        "grpd['groups_pages'] = grpd.apply(lambda x: label_bands(x), axis=1)\n",
        "df = df.merge(right=grpd.reset_index()[['page', 'groups_pages']], on=\"page\", how=\"inner\")"
      ],
      "metadata": {
        "id": "Id2X5Uu74uzF"
      },
      "id": "Id2X5Uu74uzF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next updates, I will show you a much better way. The example above is using arbitrary numbers to show you how it's done.\n",
        "\n",
        "In practice, you want something more reliable like percentiles to spot the top 1%, 20%, 50% etc.\n",
        "\n",
        "Since the top 20% would also include the top 1%, you have to be careful when doing this. Don't worry, I will explain how in the next versions."
      ],
      "metadata": {
        "id": "bRG2_cX_5gNK"
      },
      "id": "bRG2_cX_5gNK"
    },
    {
      "cell_type": "code",
      "source": [
        "# you never know...\n",
        "unique_df = df.drop_duplicates(subset=\"page\")\n",
        "\n",
        "# select \"page\" and \"groups_pages\" columns\n",
        "result = unique_df[[\"page\", \"groups_pages\"]]"
      ],
      "metadata": {
        "id": "HlMSHjmJ7vDG"
      },
      "id": "HlMSHjmJ7vDG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's color the groups! You can change the values as you wish but be careful: everything has a meaning.\n",
        "color_map = {'Weak': '#FF6347', 'Dead': '#FF0000',\n",
        "'Top': '#32CD32', 'Good': '#7CFC00',\n",
        "'Fair': '#ADFF2F', 'Opportunity': '#9ACD32'}\n",
        "\n",
        "# Create a histogram\n",
        "fig = px.histogram(\n",
        "    x=result['groups_pages'],\n",
        "    color=result['groups_pages'],\n",
        "    color_discrete_map=color_map,\n",
        "    #nbins=len(counts),\n",
        "    title='Page Classes Based on Performance',\n",
        "    labels={'x': 'Page Groups', 'y': 'Frequency'},\n",
        "    template='plotly_dark',\n",
        "    color_discrete_sequence=px.colors.qualitative.Dark2,\n",
        ")\n",
        "\n",
        "# Customize the layout\n",
        "fig.update_layout(\n",
        "    font_family='Inter',\n",
        "    legend=dict(\n",
        "        title=None, orientation='h', y=1, yanchor='bottom', x=0.5, xanchor='center',\n",
        "    ),\n",
        "    xaxis=dict(categoryorder='total descending')\n",
        ")\n",
        "\n",
        "# Show the histogram\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "YV6jwRL86K5n"
      },
      "id": "YV6jwRL86K5n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want some text to copy and paste in your report (maybe in a bullet list?), look no further."
      ],
      "metadata": {
        "id": "vHNFcIihleaW"
      },
      "id": "vHNFcIihleaW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the class values to iterate over\n",
        "class_values = ['Opportunity', 'Weak', 'Good', 'Fair', 'Top', 'Dead']\n",
        "total_pages = len(result)\n",
        "\n",
        "# Iterate over the class values and print the results for each one\n",
        "for class_value in class_values:\n",
        "    # Calculate the number of pages in the current class value\n",
        "    class_pages = len(result[result['groups_pages'] == class_value])\n",
        "\n",
        "    # Calculate the percentage of pages in the current class value out of the total\n",
        "    percent_class = (class_pages / total_pages) * 100\n",
        "\n",
        "    # Print the result using an f-string\n",
        "    print(f\"{class_pages} out of {total_pages} pages ({percent_class:.2f}%) are {class_value}.\")\n"
      ],
      "metadata": {
        "id": "JflT9zN46RjX"
      },
      "id": "JflT9zN46RjX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "48ae091b-4a9d-4e1c-9d54-afe80e538db2",
      "metadata": {
        "id": "48ae091b-4a9d-4e1c-9d54-afe80e538db2"
      },
      "source": [
        "# Content Decay (Use Case 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Content Decay is quite simple to score, you just need to load the same data as before.\n",
        "\n",
        "This part is not separate, it actually continues what's done before!\n"
      ],
      "metadata": {
        "id": "jSpjnL7LGiz_"
      },
      "id": "jSpjnL7LGiz_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1ef65b5-90be-4e73-be6e-597ac0afbf9c",
      "metadata": {
        "id": "d1ef65b5-90be-4e73-be6e-597ac0afbf9c"
      },
      "outputs": [],
      "source": [
        "#create new time variables\n",
        "df['month'] = df.date.dt.month\n",
        "df['day'] = df.date.dt.day\n",
        "#grouping and aggregating clicks\n",
        "g = df.groupby(['page','month']).agg({'clicks': 'sum'})\n",
        "#now you have a list of non-duplicate pages\n",
        "unique_pages = list(g.reset_index().page.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c955b89-9abb-4fda-a403-8681c4838f8f",
      "metadata": {
        "id": "2c955b89-9abb-4fda-a403-8681c4838f8f"
      },
      "outputs": [],
      "source": [
        "#initialize empty list\n",
        "container = []\n",
        "#loop over the list unique_pages with an index\n",
        "for i, page in enumerate(unique_pages):\n",
        "    res = {} #initialize empty dictionary\n",
        "    subset = g.loc[page]\n",
        "    x = list(subset.index)\n",
        "    y = subset.clicks.values\n",
        "\n",
        "    # fit model\n",
        "    try:\n",
        "        slope, intercept = np.poly1d(np.polyfit(x, y, 1)) #get slope and intercept of the line\n",
        "        slope = round(slope, 2) #round slope values to 2 digits\n",
        "    except: #if there is an error, set the slope to None\n",
        "        slope = None\n",
        "        intercept = None\n",
        "\n",
        "    #assign values to the empty dictionary we initialited before\n",
        "    res['page'] = page\n",
        "    res['slope'] = slope\n",
        "    container.append(res)\n",
        "\n",
        "#convert the dictionary into a dataframe\n",
        "slope_df = pd.DataFrame(container).sort_values(by='slope', ascending=True)\n",
        "slope_df.head(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disclaimer: if you get the following error: \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: RankWarning:\n",
        "\n",
        "Polyfit may be poorly conditioned\"\n",
        "\n",
        "is fine!\n",
        "\n",
        "Python is just telling you that the line is not based on enough data points, which is not a problem for our scenario.\n",
        "\n",
        "We are just interested in pages displaying extremely negative values.\n",
        "\n",
        "A better alternative is using quantiles and finding the top 10% of pages."
      ],
      "metadata": {
        "id": "xgzGxwfKEZhU"
      },
      "id": "xgzGxwfKEZhU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6145a65-04f5-4975-b95d-dcff27b8a228",
      "metadata": {
        "id": "e6145a65-04f5-4975-b95d-dcff27b8a228"
      },
      "outputs": [],
      "source": [
        "#arbitrary value, can use what you want\n",
        "len(slope_df[slope_df['slope'] < -10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "771cde50-cea4-4dd9-9958-5373eb1e29d5",
      "metadata": {
        "id": "771cde50-cea4-4dd9-9958-5373eb1e29d5"
      },
      "outputs": [],
      "source": [
        "#save your dataframe to CSV\n",
        "slope_df.reset_index().head(30).to_csv(\"decaying_pages.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
